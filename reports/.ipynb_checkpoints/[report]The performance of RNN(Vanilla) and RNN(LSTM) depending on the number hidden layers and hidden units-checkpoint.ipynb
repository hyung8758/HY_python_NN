{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The performance  of RNN(Vanilla) and RNN(LSTM) depending on the number hidden layers and hidden units.\n",
    "                                                                                        Hyungwon Yang\n",
    "                                                                                             04.21.17\n",
    "                                                                                            EMCS Labs\n",
    "### Task\n",
    "Hidden layer와 Hidden unit의 개수를 조정하여 각각 RNN(Vanilla)와 RNN(LSTM)에서의 성능 변화를 관찰 및 비교한다.\n",
    "- 영어 character 단위의 데이터셋을 이용하여 훈련한 뒤, 훈련에 사용하지 않은 테스트 셋으로 결과를 추출하여 두 모델 성능을 비교한다.\n",
    "- 본 실험에서 결과에 영향을 주는 요인은 2가지로 각각 Hidden layer의 개수와 Hidden unit의 개수이다. \n",
    " 1. Hidden layer (3가지) : 1개, 2개, 3개\n",
    " 2. Hidden unit  (5가지) : 100개, 200개, 300개, 400개, 500개\n",
    "- 위 조합에서 나올 수 있는 경우의 수는 총 15가지가 되며, 이 15가지의 경우의 수가 RNN(Vanilla)와 RNN(LSTM)에서 각각 적용되어 훈련되었다.\n",
    "\n",
    "\n",
    "### Training Corpus\n",
    "- Project Gutenberg's The Divine Comedy, Complete, by Dante Alighieri\n",
    "This eBook is for the use of anyone anywhere at no cost and with almost no restrictions whatsoever. You may copy it, give it away or re-use it under the terms of the Project Gutenberg License included with this eBook or online at www.gutenberg.org\n",
    "The part of the corpus was extracted for training.\n",
    "\n",
    "### Experimental Setting.\n",
    "- Python 3.6.0\n",
    "- Tnesorflow 1.0.1\n",
    "- Ubuntu 16.04.2\n",
    "\n",
    "### Data Preprocessing.\n",
    "- 이전 report에서 보고하였던 것으로 갈음한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN(Vanilla) Training\n",
    "\n",
    "- Hidden layer와 Hidden unit의 개수를 변경하며 실험을 진행했으며, 자세한 설정사항은 아래와 같다.\n",
    "- 설정값\n",
    " 1. 훈련에 사용된 데이터: 8,500 - 20 - 38 (# of examples, # of time steps ,# of input features)\n",
    " 2. 테스트에 사용된 데이터 : 1,650 - 20 - 38 (# of examples, # of time steps ,# of input features)\n",
    " 3. 본 실험에서는 사용하지 않았지만 리포트 상에서는 Accuracy의 변화를 보여주고자 훈련에 사용되는 데이터중 20%를 validation 셋(1,700개)으로 구성하였다. 이 validation은 epoch가 진행됨에 따라 변화되는 accuracy(인풋 케릭터에 대한 아웃풋 케릭터 결과)를 보여준다. \n",
    " 4. Parameters\n",
    "   * Epoch        : 200 (고정) \n",
    "   * Learning Rate: 0.001 \n",
    "   * Cost Function: AdamOptimizer\n",
    "   * Dropout      : on\n",
    "   * The number of hidden layers and hidden units:\n",
    "   \n",
    "|           Trials           |  1  |  2  |  3  |  4  |  5  |  6  |  7  |  8  |  9  |  10 |  11 |  12 |  13 |  14 |  15 |\n",
    "|:--------------------------:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n",
    "| The number of hidden layer |  1  |  1  |  1  |  1  |  1  |  2  |  2  |  2  |  2  |  2  |  3  |  3  |  3  |  3  |  3  |\n",
    "| The number of hidden units | 100 | 200 | 300 | 400 | 500 | 100 | 200 | 300 | 400 | 500 | 100 | 200 | 300 | 400 | 500 |\n",
    "\n",
    "- 참고사항\n",
    "  * 리포트 상의 코드(Jupyter notebook)는 훈련 진행의 변화를 쉽게 인지하도록 임의의 설정값을 이용해 진행하였으며, 본 실험과는 차이가 있을 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "# HY_python_NN absolute directory.\n",
    "my_absdir = \"/Users/hyungwonyang/Google_Drive/Python/HY_python_NN\"\n",
    "sys.path.append(my_absdir)\n",
    "\n",
    "import numpy as np\n",
    "import main.setvalues as set\n",
    "import main.rnnnetworkmodels as net\n",
    "\n",
    "# import data.\n",
    "# data directory.\n",
    "rnn_data = np.load(my_absdir+'/train_data/pg8800_lstm_char_data.npz')\n",
    "train_input = rnn_data['train_input']\n",
    "train_output = rnn_data['train_output']\n",
    "test_input = rnn_data['test_input']\n",
    "test_output = rnn_data['test_output']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# parameters\n",
    "problem = 'classification' # classification, regression\n",
    "rnnCell = 'rnn' # rnn, lstm, gru\n",
    "trainEpoch = 20\n",
    "learningRate = 0.001\n",
    "learningRateDecay = 'off' # on, off\n",
    "batchSize = 100\n",
    "dropout = 'on'\n",
    "hiddenLayers = [200]\n",
    "timeStep = 20\n",
    "costFunction = 'adam' # gradient, adam\n",
    "validationCheck = 'on' # if validationCheck is on, then 20% of train data will be taken for validation.\n",
    "\n",
    "rnn_values = set.RNNParam(inputData=train_input,\n",
    "                           targetData=train_output,\n",
    "                           timeStep=timeStep,\n",
    "                           hiddenUnits=hiddenLayers\n",
    "                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Setting hidden layers: weightMatrix and biasMatrix\n",
    "rnn_weightMatrix = rnn_values.genWeight()\n",
    "rnn_biasMatrix = rnn_values.genBias()\n",
    "rnn_input_x,rnn_input_y = rnn_values.genSymbol()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## RNN Setting #########\n",
      "Task          : classification\n",
      "Cell Type     : rnn\n",
      "Hidden Layers : 1\n",
      "Hidden Units  : [200]\n",
      "Train Epoch   : 20\n",
      "Learning Rate : 0.001\n",
      "Time Steps    : 20\n",
      "Batch Size    : 100\n",
      "Drop Out      : on\n",
      "Validation    : on\n",
      "########## RNN Setting #########\n",
      "RNN structure is generated.\n"
     ]
    }
   ],
   "source": [
    "rnn_net = net.RNNModel(inputSymbol=rnn_input_x,\n",
    "                        outputSymbol=rnn_input_y,\n",
    "                        rnnCell=rnnCell,\n",
    "                        problem=problem,\n",
    "                        hiddenLayer=hiddenLayers,\n",
    "                        trainEpoch=trainEpoch,\n",
    "                        learningRate=learningRate,\n",
    "                        learningRateDecay=learningRateDecay,\n",
    "                        timeStep=timeStep,\n",
    "                        batchSize=batchSize,\n",
    "                        dropout=dropout,\n",
    "                        validationCheck=validationCheck,\n",
    "                        weightMatrix=rnn_weightMatrix,\n",
    "                        biasMatrix=rnn_biasMatrix)\n",
    "\n",
    "# Generate a RNN(vanilla) network.\n",
    "rnn_net.genRNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activating training process.\n",
      "Epoch:   1 /  20, Cost : 2.875499, Validation Accuracy: 34.77%\n",
      "Epoch:   2 /  20, Cost : 2.270617, Validation Accuracy: 35.88%\n",
      "Epoch:   3 /  20, Cost : 2.196919, Validation Accuracy: 36.14%\n",
      "Epoch:   4 /  20, Cost : 2.167982, Validation Accuracy: 36.26%\n",
      "Epoch:   5 /  20, Cost : 2.152596, Validation Accuracy: 36.42%\n",
      "Epoch:   6 /  20, Cost : 2.143033, Validation Accuracy: 36.56%\n",
      "Epoch:   7 /  20, Cost : 2.136388, Validation Accuracy: 36.61%\n",
      "Epoch:   8 /  20, Cost : 2.131507, Validation Accuracy: 36.64%\n",
      "Epoch:   9 /  20, Cost : 2.127721, Validation Accuracy: 36.77%\n",
      "Epoch:  10 /  20, Cost : 2.124676, Validation Accuracy: 36.78%\n",
      "Epoch:  11 /  20, Cost : 2.122190, Validation Accuracy: 36.75%\n",
      "Epoch:  12 /  20, Cost : 2.120118, Validation Accuracy: 36.76%\n",
      "Epoch:  13 /  20, Cost : 2.118306, Validation Accuracy: 36.79%\n",
      "Epoch:  14 /  20, Cost : 2.116636, Validation Accuracy: 36.83%\n",
      "Epoch:  15 /  20, Cost : 2.115082, Validation Accuracy: 36.83%\n",
      "Epoch:  16 /  20, Cost : 2.113659, Validation Accuracy: 36.74%\n",
      "Epoch:  17 /  20, Cost : 2.112365, Validation Accuracy: 36.73%\n",
      "Epoch:  18 /  20, Cost : 2.111178, Validation Accuracy: 36.74%\n",
      "Epoch:  19 /  20, Cost : 2.110079, Validation Accuracy: 36.75%\n",
      "Epoch:  20 /  20, Cost : 2.109053, Validation Accuracy: 36.75%\n",
      "The model has been trained successfully.\n"
     ]
    }
   ],
   "source": [
    "# Train the RNN(vanilla) network.\n",
    "# In this tutorial, we will run only 20 epochs.\n",
    "rnn_net.trainRNN(train_input,train_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activating Testing Process\n",
      "Tested with 1650 datasets.\n",
      "Test Accuracy: 37.40 %\n"
     ]
    }
   ],
   "source": [
    "# Test the trained RNN(vanilla) network.\n",
    "rnn_net.testRNN(test_input,test_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable list as a dictionary format.\n",
      ">> weight, bias, y_hat, optimizer, cost\n",
      "\n",
      "RNN training session is terminated.\n"
     ]
    }
   ],
   "source": [
    "# Save the trained parameters.\n",
    "vars = rnn_net.getVariables()\n",
    "# Terminate the session.\n",
    "rnn_net.closeRNN()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN(LSTM) Training\n",
    "\n",
    "- Hidden layer와 Hidden unit의 개수를 변경하며 실험을 진행했으며, 자세한 설정사항은 아래와 같다.\n",
    "- 설정값\n",
    " 1. 훈련에 사용된 데이터: 8,500 - 20 - 38 (# of examples, # of time steps ,# of input features)\n",
    " 2. 테스트에 사용된 데이터 : 1,650 - 20 - 38 (# of examples, # of time steps ,# of input features)\n",
    " 3. 본 실험에서는 사용하지 않았지만 리포트 상에서는 Accuracy의 변화를 보여주고자 훈련에 사용되는 데이터중 20%를 validation 셋(1,700개)으로 구성하였다. 이 validation은 epoch가 진행됨에 따라 변화되는 accuracy(인풋 케릭터에 대한 아웃풋 케릭터 결과)를 보여준다. \n",
    " 4. Parameters\n",
    "   * Epoch        : 200 (고정) \n",
    "   * Learning Rate: 0.001 \n",
    "   * Dropout      : on\n",
    "   * Cost Function: AdamOptimizer\n",
    "   * The number of hidden layers and hidden units:\n",
    "   \n",
    "|           Trials           |  1  |  2  |  3  |  4  |  5  |  6  |  7  |  8  |  9  |  10 |  11 |  12 |  13 |  14 |  15 |\n",
    "|:--------------------------:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n",
    "| The number of hidden layer |  1  |  1  |  1  |  1  |  1  |  2  |  2  |  2  |  2  |  2  |  3  |  3  |  3  |  3  |  3  |\n",
    "| The number of hidden units | 100 | 200 | 300 | 400 | 500 | 100 | 200 | 300 | 400 | 500 | 100 | 200 | 300 | 400 | 500 |\n",
    "\n",
    "- 참고사항\n",
    "  * 리포트 상의 코드(Jupyter notebook)는 훈련 진행의 변화를 쉽게 인지하도록 임의의 설정값을 이용해 진행하였으며, 본 실험과는 차이가 있을 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import main.setvalues as set\n",
    "import main.rnnnetworkmodels as net\n",
    "\n",
    "# import data.\n",
    "# data directory.\n",
    "lstm_data = np.load(my_absdir+'/train_data/pg8800_lstm_char_data.npz')\n",
    "train_input = lstm_data['train_input']\n",
    "train_output = lstm_data['train_output']\n",
    "test_input = lstm_data['test_input']\n",
    "test_output = lstm_data['test_output']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# parameters\n",
    "problem = 'classification' # classification, regression\n",
    "rnnCell = 'lstm' # rnn, lstm, gru\n",
    "trainEpoch = 20\n",
    "learningRate = 0.001\n",
    "learningRateDecay = 'off' # on, off\n",
    "batchSize = 100\n",
    "dropout = 'on'\n",
    "hiddenLayers = [200]\n",
    "timeStep = 20\n",
    "costFunction = 'adam' # gradient, adam\n",
    "validationCheck = 'on' # if validationCheck is on, then 20% of train data will be taken for validation.\n",
    "\n",
    "lstm_values = set.RNNParam(inputData=train_input,\n",
    "                           targetData=train_output,\n",
    "                           timeStep=timeStep,\n",
    "                           hiddenUnits=hiddenLayers\n",
    "                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Setting hidden layers: weightMatrix and biasMatrix\n",
    "lstm_weightMatrix = lstm_values.genWeight()\n",
    "lstm_biasMatrix = lstm_values.genBias()\n",
    "lstm_input_x,lstm_input_y = lstm_values.genSymbol()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## RNN Setting #########\n",
      "Task          : classification\n",
      "Cell Type     : lstm\n",
      "Hidden Layers : 1\n",
      "Hidden Units  : [200]\n",
      "Train Epoch   : 20\n",
      "Learning Rate : 0.001\n",
      "Time Steps    : 20\n",
      "Batch Size    : 100\n",
      "Drop Out      : on\n",
      "Validation    : on\n",
      "########## RNN Setting #########\n",
      "RNN structure is generated.\n"
     ]
    }
   ],
   "source": [
    "lstm_net = net.RNNModel(inputSymbol=lstm_input_x,\n",
    "                        outputSymbol=lstm_input_y,\n",
    "                        rnnCell=rnnCell,\n",
    "                        problem=problem,\n",
    "                        hiddenLayer=hiddenLayers,\n",
    "                        trainEpoch=trainEpoch,\n",
    "                        learningRate=learningRate,\n",
    "                        learningRateDecay=learningRateDecay,\n",
    "                        timeStep=timeStep,\n",
    "                        batchSize=batchSize,\n",
    "                        dropout=dropout,\n",
    "                        validationCheck=validationCheck,\n",
    "                        weightMatrix=lstm_weightMatrix,\n",
    "                        biasMatrix=lstm_biasMatrix)\n",
    "\n",
    "# Generate a RNN(lstm) network.\n",
    "lstm_net.genRNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activating training process.\n",
      "Epoch:   1 /  20, Cost : 3.000563, Validation Accuracy: 29.29%\n",
      "Epoch:   2 /  20, Cost : 2.487978, Validation Accuracy: 32.98%\n",
      "Epoch:   3 /  20, Cost : 2.325643, Validation Accuracy: 34.48%\n",
      "Epoch:   4 /  20, Cost : 2.245832, Validation Accuracy: 35.90%\n",
      "Epoch:   5 /  20, Cost : 2.192866, Validation Accuracy: 36.53%\n",
      "Epoch:   6 /  20, Cost : 2.151267, Validation Accuracy: 37.42%\n",
      "Epoch:   7 /  20, Cost : 2.113395, Validation Accuracy: 38.35%\n",
      "Epoch:   8 /  20, Cost : 2.078679, Validation Accuracy: 38.96%\n",
      "Epoch:   9 /  20, Cost : 2.047445, Validation Accuracy: 39.58%\n",
      "Epoch:  10 /  20, Cost : 2.019518, Validation Accuracy: 40.11%\n",
      "Epoch:  11 /  20, Cost : 1.994652, Validation Accuracy: 40.54%\n",
      "Epoch:  12 /  20, Cost : 1.972219, Validation Accuracy: 41.04%\n",
      "Epoch:  13 /  20, Cost : 1.951622, Validation Accuracy: 41.60%\n",
      "Epoch:  14 /  20, Cost : 1.932475, Validation Accuracy: 41.99%\n",
      "Epoch:  15 /  20, Cost : 1.914544, Validation Accuracy: 42.33%\n",
      "Epoch:  16 /  20, Cost : 1.897514, Validation Accuracy: 42.56%\n",
      "Epoch:  17 /  20, Cost : 1.881204, Validation Accuracy: 42.83%\n",
      "Epoch:  18 /  20, Cost : 1.865619, Validation Accuracy: 43.11%\n",
      "Epoch:  19 /  20, Cost : 1.850687, Validation Accuracy: 43.43%\n",
      "Epoch:  20 /  20, Cost : 1.836316, Validation Accuracy: 43.67%\n",
      "The model has been trained successfully.\n"
     ]
    }
   ],
   "source": [
    "# Train the RNN(lstm) network.\n",
    "# In this tutorial, we will run only 20 epochs.\n",
    "lstm_net.trainRNN(train_input,train_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activating Testing Process\n",
      "Tested with 1650 datasets.\n",
      "Test Accuracy: 45.30 %\n"
     ]
    }
   ],
   "source": [
    "# Test the trained RNN(lstm) network.\n",
    "lstm_net.testRNN(test_input,test_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable list as a dictionary format.\n",
      ">> weight, bias, y_hat, optimizer, cost\n",
      "\n",
      "RNN training session is terminated.\n"
     ]
    }
   ],
   "source": [
    "# Save the trained parameters.\n",
    "vars = lstm_net.getVariables()\n",
    "# Terminate the session.\n",
    "lstm_net.closeRNN()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments\n",
    "- 본 실험에서 Hidden layer와 Hidden Unit 개수를 각각 달리하여 훈련을 진행하였으며, 이 두가지 요인의 변화에 따른 성능변화를 알아보고자 한다.\n",
    "- 위 실험에서 언급한 파라미터값은 모든 경우의 실험 가지수에서 동일하게 설정하고 진행하였다.\n",
    "\n",
    "### Result\n",
    "1. 그래프상에서 RNN(Vanilla)와 RNN(LSTM)간에 훈련 성능차가 뚜렷하게 나타나고 있다. \n",
    "2. RNN(Vanilla)의 경우 Hidden Unit이 100과 200일 때 Hidden layer의 개수가 늘어남에 따라 성능이 좋아지는 것을 볼 수 있다. 하지만 Hidden unit의 개수가 늘어나자 300이상으로 늘어날 경우 Hidden layer의 개수와 상관없이 성능이 악화되고 있다. 이는 기계학습에서 전형적으로 나타나는 Overfitting의 문제로 볼 수 있는데, 비록 본 리포트에서는 보고하지 않았지만, RNN(Vanilla)을 통한 데이터 훈련 시 Error값은 계속해서 낮아지고 있었고,  훈련에 사용하지 않은 Validation 세트를 통해 확인해 본 Accuracy는 점차 나빠지고 있었다는 점을 통해 유추할 수 있다.\n",
    "3. RNN(LSTM)의 경우 RNN(Vanilla)에서와 동일하게 Hidden unit의 개수가 적은 경우 Hidden layer의 개수와 비례해서 성능이 향상되는 양상을 보인다. 하지만 Hidden unit의 개수 자체가 300이상으로 증가했을 경우에는 Hidden layer의 개수와 상관없이 높은 성능 향상을 보여주고 있다. 그리고 RNN(Vanilla)와는 다르게 Hidden layer의 개수가 증가하더라도 ceilling effect만 보일 뿐, Overfitting으로 인한 Accuracy의 하락은 보이지 않는다. \n",
    "\n",
    "<br/>\n",
    "<center>PERFORMANCE TABLE</center>\n",
    "\n",
    "|           Trials           |   1   |   2   |   3   |   4   |   5   |   6   |   7   |   8   |   9   |   10  |   11  |   12  |   13  |   14  |   15  |\n",
    "|:--------------------------:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|\n",
    "| The number of hidden layer |   1   |   1   |   1   |   1   |   1   |   2   |   2   |   2   |   2   |   2   |   3   |   3   |   3   |   3   |   3   |\n",
    "| The number of hidden units |  100  |  200  |  300  |  400  |  500  |  100  |  200  |  300  |  400  |  500  |  100  |  200  |  300  |  400  |  500  |\n",
    "| RNN(Vanilla) (Accuracy%)   | 48.13 | 50.09 | 43.29 | 41.72 | 39.77 | 52.53 | 54.85 | 48.23 | 41.47 | 40.36 | 55.93 | 58.55 | 48.53 | 43.76 | 40.42 |\n",
    "| RNN(LSTM) (Accuracy%)      | 56.57 | 72.46 | 86.95 | 87.68 | 87.55 | 65.62 | 86.82 | 87.29 | 87.43 | 86.98 | 71.22 | 86.59 | 87.08 | 87.37 | 87.49 |\n",
    "\n",
    "<br/>\n",
    "<center>PERFORMANCE GRAPH</center>\n",
    "\n",
    "![enter image description here](https://lh3.googleusercontent.com/-4FzZzz2QgHk/WPsamz-HqrI/AAAAAAAAEjc/fitgPT4jJD8hhzEIpi5rH1UYQIAAcVbJwCLcB/s500/Screen+Shot+2017-04-22+at+5.55.05+PM.png \"Screen Shot 2017-04-22 at 5.55.05 PM.png\")\n",
    "\n",
    "### Further Task\n",
    "1. RNN(LSTM)의 성능 향상을 위해 Dropout을 적용하고, 다른 Costfunction을 사용해 보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Github Code\n",
    "\n",
    "- 다음의 깃헙 코드를 다운받으면 본 실험을 재현할 수 있다.\n",
    " - https://github.com/hyung8758/HY_python_NN.git\n",
    "- Jupyter에서 실행을 원할 경우, 위 코드를 받고 jupyter notebook 최상단쯤에 보이는 absolute directory를 코드 상의 폴더이름(/your/path/to/HY_python_NN)으로 정한 뒤 실행하면 된다.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
