{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Performance by varing the size of time steps.\n",
    "                                                                                        Hyungwon Yang\n",
    "                                                                                             05.15.17\n",
    "                                                                                            EMCS Labs\n",
    "### Purpose\n",
    "Recurrent neural network learns time information from the dataset becuase it feeds past information from a previous hidden layer. This history is accumulated in the hidden layers as time step (the step how much time information will RNN take into account) goes by in the training session and it helps the RNN to handle time related tasks more efficiently than other types of neural networks. This RNN learns the time information more proficiently when it selectes LSTM cell, which provides more gates in the RNN cell and its powerful performance was proved by many researchers because it truly improves prediction accuracy especially when it comes to dealing with the tasks that hold temporal information such as language. Today, we will use LSTM based RNN to train acoustic information(speech) to map them to articulatory information(tongue movement). In this experiment, Most of the parameters such as the number hidden layers and units, or learning rate will be fixed and only time step will vary.\n",
    "\n",
    "### Task\n",
    "- Train the acoustics to articulations mapping datasets with the LSTM RNN by varing the time step parameter.\n",
    "- Evaluating the performance of trained models with articulation estimator.\n",
    "\n",
    "### Datasets\n",
    "- XRBM datasets were used for training and testing.\n",
    "- Input: mfcc values extracted from speech data. \n",
    "- Output: x and y positional informations from pallets that attached to tongue, teeth, and lips.\n",
    "\n",
    "### Prameters\n",
    "- The number of hidden layer: 1\n",
    "- The number of hidden units: 300\n",
    "- Epoch: 1000\n",
    "- Learning rate: 0.00001\n",
    "- batch size: 10\n",
    "- Time step: 2,3,4,5,6,7,10,20,30,40,50,110\n",
    "\n",
    "### Training Code\n",
    "- RNN: Written by Heejo You in python (reference: Alex Graves, *Generating Sequences With Recurrent Neural Networks*)\n",
    "- Articulation estimator: Written by Jaekoo Kang in matlab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Minimum error(MSE) at 1000 epoch.\n",
    "# Time step 2: 0.13144\n",
    "# Time step 3: 0.19344\n",
    "# Time step 4: 0.25463\n",
    "# Time step 5: 0.31467\n",
    "# Time step 6: 0.37360\n",
    "# Time step 7: 0.43291\n",
    "# Time step 10: 0.58055\n",
    "# Time step 20: 0.78164\n",
    "# Time step 30: 0.\n",
    "# Time step 40: 0.\n",
    "# Time step 50: 0.\n",
    "# Time step 110: 0.84920"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
