{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of English character recognition performance between ANN and RNN(LSTM).\n",
    "\n",
    "                                                                                Hyungwon Yang\n",
    "                                                                                     04.19.17\n",
    "                                                                                    EMCS Labs\n",
    "\n",
    "\n",
    "### Task \n",
    "Compare the performance achieved by two machine learning techniques: ANN and RNN(LSTM).\n",
    "Train the preprocessed character level corpus, *Project Gutenberg's The Divine Comedy, Complete ebook*, with the two techniques and test their performance.\n",
    "\n",
    "### Training Corpus\n",
    "Project Gutenberg's The Divine Comedy, Complete, by Dante Alighieri\n",
    "\n",
    "This eBook is for the use of anyone anywhere at no cost and with\n",
    "almost no restrictions whatsoever.  You may copy it, give it away or\n",
    "re-use it under the terms of the Project Gutenberg License included\n",
    "with this eBook or online at www.gutenberg.org\n",
    "\n",
    "The part of the corpus was extracted for training.\n",
    "\n",
    "### Experimental Setting.\n",
    "1. Python 3.5.3\n",
    "2. Tnesorflow 1.0.0\n",
    "3. Mac OSX sierra 10.12.4\n",
    "\n",
    "### Data Preprocessing.\n",
    "데이터 정제와 관련된 사항을 적었음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Data preprocessing.\n",
    "This part shows how I preprocessed the text data.\n",
    "You don't have to run this code becuase this process is time consuming.\n",
    "If you want to download the data file, find and run downloader.py in a HY_python_NN folder.\n",
    "'''\n",
    "\n",
    "# Data preparation for ANN & LSTM performance comparison.\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "with open('train_data/pg8800_train','r') as train_n:\n",
    "    train_ngram = np.loadtxt(train_n.readlines(),dtype=int)\n",
    "\n",
    "with open('train_data/pg8800_test','r') as test_n:\n",
    "    test_ngram = np.loadtxt(test_n.readlines(),dtype=int)\n",
    "\n",
    "with open('train_data/pg8800_words','r') as look_w:\n",
    "    lookup = look_w.readlines()\n",
    "    lookup_words = []\n",
    "    for string in lookup:\n",
    "        lookup_words.append(re.sub('\\n','',string))\n",
    "\n",
    "vocab_size = len(lookup_words)\n",
    "train_data_size = len(train_ngram)\n",
    "test_data_size = len(test_ngram)\n",
    "\n",
    "\n",
    "##################################################################################\n",
    "### word level\n",
    "# This is first method. combine 3-gram. (1*5000) * 3 = 1 * 15000\n",
    "# This part is not written as a code.\n",
    "\n",
    "# This is second method. put 3-gram into 1 vocabulary.\n",
    "## ANN\n",
    "ann_train_inputs = np.zeros((train_data_size,vocab_size))\n",
    "ann_train_outputs = np.zeros((train_data_size,vocab_size))\n",
    "box = 0\n",
    "for idx in train_ngram:\n",
    "    ann_train_inputs[box][idx[0:3]] = 1\n",
    "    ann_train_outputs[box][idx[-1]] = 1\n",
    "    box += 1\n",
    "# test_inputs\n",
    "ann_test_inputs = np.zeros((test_data_size,vocab_size))\n",
    "ann_test_outputs = np.zeros((test_data_size,vocab_size))\n",
    "box = 0\n",
    "for idx in test_ngram:\n",
    "    ann_test_inputs[box][idx[0:3]] = 1\n",
    "    ann_train_outputs[box][idx[-1]] = 1\n",
    "    box += 1\n",
    "\n",
    "\n",
    "## LSTM\n",
    "timeStep = 3\n",
    "\n",
    "# dictionary list.\n",
    "# Be cautious. This takes a lot of time to generate data.\n",
    "word_box = np.identity(len(lookup_words),dtype=int)\n",
    "input_box = np.zeros((timeStep,vocab_size))\n",
    "lstm_train_inputs = np.empty((1,timeStep,vocab_size))\n",
    "lstm_train_outputs = np.empty((1,timeStep,vocab_size))\n",
    "con=0\n",
    "for idx in train_ngram:\n",
    "    input_box = np.zeros((timeStep,vocab_size))\n",
    "    output_box = np.zeros((timeStep, vocab_size))\n",
    "    for input in list(range(timeStep)):\n",
    "        input_box[input][idx[input]] = 1\n",
    "    for output in list(range(timeStep)):\n",
    "        output_box[output][idx[output+1]] = 1\n",
    "    lstm_train_inputs = np.append(lstm_train_inputs,[input_box],axis=0)\n",
    "    lstm_train_outputs = np.append(lstm_train_outputs, [output_box], axis=0)\n",
    "    con+=1\n",
    "    if con % 500 == 0:\n",
    "        print ('{} / {} is completed'.format(con,train_data_size))\n",
    "\n",
    "\n",
    "\n",
    "##################################################################################\n",
    "### char level\n",
    "# data preprocessing.\n",
    "# import data.\n",
    "with open('train_data/pg8800_train_chars','r') as train_n:\n",
    "    train_char = train_n.readlines()\n",
    "    tmp_train = train_char[0].split(' ')\n",
    "    train_split_char = tmp_train[0:170000]\n",
    "\n",
    "with open('train_data/pg8800_test_chars','r') as test_n:\n",
    "    test_char = test_n.readlines()\n",
    "    tmp_test = test_char[0].split(' ')\n",
    "    test_split_char = tmp_train[0:33000]\n",
    "\n",
    "with open('train_data/pg8800_char_list','r') as look_w:\n",
    "    lookup = look_w.readlines()\n",
    "    lookup_chars = []\n",
    "    for string in lookup:\n",
    "        lookup_chars.append((re.sub('\\n','',string)))\n",
    "\n",
    "\n",
    "vocab_size = len(lookup_chars)\n",
    "train_data_size = len(train_split_char)\n",
    "test_data_size = len(test_split_char)\n",
    "\n",
    "# data digitizing.\n",
    "## ANN\n",
    "# train data\n",
    "ann_train_input_char = np.zeros((train_data_size-1,vocab_size))\n",
    "ann_train_output_char = np.zeros((train_data_size-1,vocab_size))\n",
    "for dat in list(range(train_data_size-1)):\n",
    "    input_sym = train_split_char[dat]\n",
    "    output_sym = train_split_char[dat+1]\n",
    "    input_idx = lookup_chars.index(input_sym)\n",
    "    output_idx = lookup_chars.index(output_sym)\n",
    "    ann_train_input_char[dat][input_idx] = 1\n",
    "    ann_train_output_char[dat][output_idx] = 1\n",
    "\n",
    "\n",
    "# test data\n",
    "ann_test_input_char = np.zeros((test_data_size - 1, vocab_size))\n",
    "ann_test_output_char = np.zeros((test_data_size - 1, vocab_size))\n",
    "for dat in list(range(test_data_size - 1)):\n",
    "    input_sym = test_split_char[dat]\n",
    "    output_sym = test_split_char[dat + 1]\n",
    "    input_idx = lookup_chars.index(input_sym)\n",
    "    output_idx = lookup_chars.index(output_sym)\n",
    "    ann_test_input_char[dat][input_idx] = 1\n",
    "    ann_test_output_char[dat][output_idx] = 1\n",
    "np.savez('train_data/pg8800_ann_char_data',train_input=ann_train_input_char,\n",
    "         train_output=ann_train_output_char,test_input=ann_test_input_char,test_output=ann_test_output_char)\n",
    "\n",
    "## LSTM\n",
    "lstm_train_input_char = np.zeros((int(train_data_size/timeStep),timeStep,vocab_size))\n",
    "lstm_train_output_char = np.zeros((int(train_data_size/timeStep),timeStep,vocab_size))\n",
    "his = 0\n",
    "for dat in list(range(int(train_data_size/timeStep)-1)):\n",
    "    for times in list(range(timeStep)):\n",
    "        input_sym = train_split_char[his]\n",
    "        output_sym = train_split_char[his+1]\n",
    "        input_idx = lookup_chars.index(input_sym)\n",
    "        output_idx = lookup_chars.index(output_sym)\n",
    "        lstm_train_input_char[dat][times][input_idx] = 1\n",
    "        lstm_train_output_char[dat][times][output_idx] = 1\n",
    "        his += 1\n",
    "\n",
    "# test data\n",
    "lstm_test_input_char = np.zeros((int(test_data_size/timeStep),timeStep,vocab_size))\n",
    "lstm_test_output_char = np.zeros((int(test_data_size/timeStep),timeStep,vocab_size))\n",
    "his = 0\n",
    "for dat in list(range(int(test_data_size/timeStep)-1)):\n",
    "    for times in list(range(timeStep)):\n",
    "        input_sym = test_split_char[his]\n",
    "        output_sym = test_split_char[his+1]\n",
    "        input_idx = lookup_chars.index(input_sym)\n",
    "        output_idx = lookup_chars.index(output_sym)\n",
    "        lstm_test_input_char[dat][times][input_idx] = 1\n",
    "        lstm_test_output_char[dat][times][output_idx] = 1\n",
    "        his += 1\n",
    "np.savez('train_data/pg8800_lstm_char_data',train_input=lstm_train_input_char,\n",
    "         train_output=lstm_train_output_char,test_input=lstm_test_input_char,test_output=lstm_test_output_char)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANN training.\n",
    "\n",
    "- Comments\n",
    " 1. 훈련에 사용된 데이터: 170,000 * 38 (# of examples, # of input features)\n",
    " 2. 테스트에 사용된 데이터 : 33,000 * 38 (# of examples, # of input features)\n",
    " 3. 훈련에 사용되는 데이터중 20%를 validation 셋으로 구성하였다. (32,000개) 이 validation은 epoch가 진행됨에 따라 변화되는 accuracy(인풋 케릭터에 대한 아웃풋 케릭터 결과)를 보여준다.\n",
    " 4. parameters: epoch: 200, 1 hidden layer and its size: 200, learning rate: 0.001, cost function: adam\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "# HY_python_NN absolute directory.\n",
    "my_absdir = \"/Users/hyungwonyang/Google_Drive/Python/HY_python_NN\"\n",
    "sys.path.append(my_absdir)\n",
    "import numpy as np\n",
    "import main.setvalues as set\n",
    "import main.dnnnetworkmodels as net\n",
    "\n",
    "\n",
    "# import data.\n",
    "# data directory.\n",
    "ann_data = np.load(my_absdir+'/train_data/pg8800_ann_char_data.npz')\n",
    "train_input = ann_data['train_input']\n",
    "train_output = ann_data['train_output']\n",
    "test_input = ann_data['test_input']\n",
    "test_output = ann_data['test_output']\n",
    "\n",
    "vocab_size = train_input.shape[1]\n",
    "train_data_size = train_input.shape[0]\n",
    "test_data_size = test_input.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# parameter setting.\n",
    "\n",
    "fineTrainEpoch = 20\n",
    "fineLearningRate = 0.001\n",
    "learningRateDecay = 'off' # on, off\n",
    "batchSize = 100\n",
    "hiddenLayers = [200]\n",
    "problem = 'classification' # classification, regression\n",
    "hiddenFunction= 'tanh'\n",
    "costFunction = 'adam' # gradient, adam\n",
    "validationCheck = 'on' # if validationCheck is on, then 20% of train data will be taken for validation.\n",
    "PlotGraph = 'off' # If this is on, graph will be saved in the rnn_graph directory.\n",
    "                  # You can check the dnn structure on the tensorboard.\n",
    "\n",
    "\n",
    "DNN_values = set.setParam(inputData=train_input,\n",
    "                    targetData=train_output,\n",
    "                    hiddenUnits=hiddenLayers\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Setting hidden layers: weightMatrix and biasMatrix\n",
    "weightMatrix = DNN_values.genWeight()\n",
    "biasMatrix = DNN_values.genBias()\n",
    "# Generating input symbols.\n",
    "input_x, input_y = DNN_values.genSymbol()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dnn = net.DNNmodel(inputSymbol=input_x,\n",
    "                   outputSymbol=input_y,\n",
    "                   problem=problem,\n",
    "                   fineTrainEpoch=fineTrainEpoch,\n",
    "                   fineLearningRate=fineLearningRate,\n",
    "                   learningRateDecay=learningRateDecay,\n",
    "                   batchSize=batchSize,\n",
    "                   hiddenFunction=hiddenFunction,\n",
    "                   costFunction=costFunction,\n",
    "                   validationCheck=validationCheck,\n",
    "                   weightMatrix=weightMatrix,\n",
    "                   biasMatrix=biasMatrix\n",
    "                   )\n",
    "                   \n",
    "# Generate a DNN network.\n",
    "dnn.genDNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1 / 20 , Cost : 2.684716\n",
      "Validation Accuracy: 28.93 %\n",
      "Epoch : 2 / 20 , Cost : 2.394949\n",
      "Validation Accuracy: 29.00 %\n",
      "Epoch : 3 / 20 , Cost : 2.373842\n",
      "Validation Accuracy: 29.00 %\n",
      "Epoch : 4 / 20 , Cost : 2.367025\n",
      "Validation Accuracy: 28.99 %\n",
      "Epoch : 5 / 20 , Cost : 2.363765\n",
      "Validation Accuracy: 28.99 %\n",
      "Epoch : 6 / 20 , Cost : 2.361784\n",
      "Validation Accuracy: 28.89 %\n",
      "Epoch : 7 / 20 , Cost : 2.360447\n",
      "Validation Accuracy: 28.89 %\n",
      "Epoch : 8 / 20 , Cost : 2.359527\n",
      "Validation Accuracy: 28.89 %\n",
      "Epoch : 9 / 20 , Cost : 2.358834\n",
      "Validation Accuracy: 28.89 %\n",
      "Epoch : 10 / 20 , Cost : 2.358280\n",
      "Validation Accuracy: 28.89 %\n",
      "Epoch : 11 / 20 , Cost : 2.357804\n",
      "Validation Accuracy: 28.89 %\n",
      "Epoch : 12 / 20 , Cost : 2.357378\n",
      "Validation Accuracy: 28.74 %\n",
      "Epoch : 13 / 20 , Cost : 2.356986\n",
      "Validation Accuracy: 28.74 %\n",
      "Epoch : 14 / 20 , Cost : 2.356620\n",
      "Validation Accuracy: 28.51 %\n",
      "Epoch : 15 / 20 , Cost : 2.356275\n",
      "Validation Accuracy: 28.51 %\n",
      "Epoch : 16 / 20 , Cost : 2.355948\n",
      "Validation Accuracy: 28.51 %\n",
      "Epoch : 17 / 20 , Cost : 2.355639\n",
      "Validation Accuracy: 28.51 %\n",
      "Epoch : 18 / 20 , Cost : 2.355346\n",
      "Validation Accuracy: 28.51 %\n",
      "Epoch : 19 / 20 , Cost : 2.355066\n",
      "Validation Accuracy: 28.51 %\n",
      "Epoch : 20 / 20 , Cost : 2.354801\n",
      "Validation Accuracy: 28.74 %\n",
      "The model has been trained successfully.\n"
     ]
    }
   ],
   "source": [
    "# Train the DNN network.\n",
    "# In this tutorial, we will run only 20 epochs.\n",
    "dnn.trainDNN(train_input,train_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tested with  32999  datasets.\n",
      "Test Accuracy: 28.09 %\n"
     ]
    }
   ],
   "source": [
    "# Test the trained DNN network.\n",
    "dnn.testDNN(test_input,test_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable list as a dictionary format.\n",
      ">> weight, bias, y_hat, optimizer, cost\n",
      "\n",
      "DNN training session is terminated.\n"
     ]
    }
   ],
   "source": [
    "# Save the trained parameters.\n",
    "vars = dnn.getVariables()\n",
    "# Terminate the session.\n",
    "dnn.closeDNN()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN(LSTM) training.\n",
    "\n",
    "- Comments\n",
    " 1. 훈련에 사용된 데이터: 8,500 * 20 * 38 (# of examples, # of time steps ,# of input features)\n",
    " 2. 테스트에 사용된 데이터 : 1,650 * 20 * 38 (# of examples, # of time steps ,# of input features)\n",
    " 3. 훈련에 사용되는 데이터중 20%를 validation 셋으로 구성하였다. (1,700개) 이 validation은 epoch가 진행됨에 따라 변화되는 \n",
    "                accuracy(인풋 케릭터에 대한 아웃풋 케릭터 결과)를 보여준다.\n",
    " 4. parameters: epoch: 200, 1 hidden layer and its size: 200, learning rate: 0.001, cost function: adam\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import main.setvalues as set\n",
    "import main.rnnnetworkmodels as net\n",
    "\n",
    "# import data.\n",
    "# data directory.\n",
    "lstm_data = np.load(my_absdir+'/train_data/pg8800_lstm_char_data.npz')\n",
    "train_input = lstm_data['train_input']\n",
    "train_output = lstm_data['train_output']\n",
    "test_input = lstm_data['test_input']\n",
    "test_output = lstm_data['test_output']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# parameters\n",
    "problem = 'classification' # classification, regression\n",
    "rnnCell = 'lstm' # rnn, lstm\n",
    "trainEpoch = 20\n",
    "learningRate = 0.001\n",
    "learningRateDecay = 'off' # on, off\n",
    "batchSize = 100\n",
    "hiddenLayers = [200]\n",
    "timeStep = 20\n",
    "costFunction = 'adam' # gradient, adam\n",
    "validationCheck = 'on' # if validationCheck is on, then 20% of train data will be taken for validation.\n",
    "\n",
    "lstm_values = set.simpleRNNParam(inputData=train_input,\n",
    "                           targetData=train_output,\n",
    "                           timeStep=timeStep,\n",
    "                           hiddenUnits=hiddenLayers\n",
    "                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Setting hidden layers: weightMatrix and biasMatrix\n",
    "lstm_weightMatrix = lstm_values.genWeight()\n",
    "lstm_biasMatrix = lstm_values.genBias()\n",
    "lstm_input_x,lstm_input_y = lstm_values.genSymbol()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN cell type is lstm\n"
     ]
    }
   ],
   "source": [
    "lstm_net = net.simpleRNNModel(inputSymbol=lstm_input_x,\n",
    "                               outputSymbol=lstm_input_y,\n",
    "                               rnnCell=rnnCell,\n",
    "                               problem=problem,\n",
    "                               trainEpoch=trainEpoch,\n",
    "                               learningRate=learningRate,\n",
    "                               timeStep=timeStep,\n",
    "                               batchSize=batchSize,\n",
    "                               validationCheck=validationCheck,\n",
    "                               weightMatrix=lstm_weightMatrix,\n",
    "                               biasMatrix=lstm_biasMatrix)\n",
    "\n",
    "# Generate a RNN(lstm) network.\n",
    "lstm_net.genRNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1 / 20 , Cost : 2.983213\n",
      "Validation Accuracy: 28.42 %\n",
      "Epoch : 2 / 20 , Cost : 2.484019\n",
      "Validation Accuracy: 32.73 %\n",
      "Epoch : 3 / 20 , Cost : 2.319190\n",
      "Validation Accuracy: 34.88 %\n",
      "Epoch : 4 / 20 , Cost : 2.240479\n",
      "Validation Accuracy: 36.06 %\n",
      "Epoch : 5 / 20 , Cost : 2.189115\n",
      "Validation Accuracy: 36.87 %\n",
      "Epoch : 6 / 20 , Cost : 2.147921\n",
      "Validation Accuracy: 37.85 %\n",
      "Epoch : 7 / 20 , Cost : 2.110794\n",
      "Validation Accuracy: 38.44 %\n",
      "Epoch : 8 / 20 , Cost : 2.076097\n",
      "Validation Accuracy: 38.98 %\n",
      "Epoch : 9 / 20 , Cost : 2.044260\n",
      "Validation Accuracy: 39.67 %\n",
      "Epoch : 10 / 20 , Cost : 2.015307\n",
      "Validation Accuracy: 40.21 %\n",
      "Epoch : 11 / 20 , Cost : 1.989075\n",
      "Validation Accuracy: 40.64 %\n",
      "Epoch : 12 / 20 , Cost : 1.965307\n",
      "Validation Accuracy: 41.06 %\n",
      "Epoch : 13 / 20 , Cost : 1.943113\n",
      "Validation Accuracy: 41.55 %\n",
      "Epoch : 14 / 20 , Cost : 1.921944\n",
      "Validation Accuracy: 42.01 %\n",
      "Epoch : 15 / 20 , Cost : 1.902267\n",
      "Validation Accuracy: 42.49 %\n",
      "Epoch : 16 / 20 , Cost : 1.883971\n",
      "Validation Accuracy: 42.86 %\n",
      "Epoch : 17 / 20 , Cost : 1.866794\n",
      "Validation Accuracy: 43.26 %\n",
      "Epoch : 18 / 20 , Cost : 1.850557\n",
      "Validation Accuracy: 43.60 %\n",
      "Epoch : 19 / 20 , Cost : 1.835138\n",
      "Validation Accuracy: 43.73 %\n",
      "Epoch : 20 / 20 , Cost : 1.820427\n",
      "Validation Accuracy: 43.96 %\n",
      "The model has been trained successfully.\n"
     ]
    }
   ],
   "source": [
    "# Train the RNN(lstm) network.\n",
    "# In this tutorial, we will run only 20 epochs.\n",
    "lstm_net.trainRNN(train_input,train_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tested with 1650 datasets.\n",
      "Test Accuracy: 45.86 %\n"
     ]
    }
   ],
   "source": [
    "# Test the trained RNN(lstm) network.\n",
    "lstm_net.testRNN(test_input,test_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable list as a dictionary format.\n",
      ">> weight, bias, y_hat, optimizer, cost\n",
      "\n",
      "Simple RNN training session is terminated.\n"
     ]
    }
   ],
   "source": [
    "# Save the trained parameters.\n",
    "vars = lstm_net.getVariables()\n",
    "# Terminate the session.\n",
    "lstm_net.closeRNN()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result\n",
    "위 코드상에서 실험은 히든레이어 유닛 개수가 200개인경우만 한정지어 진행하였으나, 실제로는 히든레이어 유닛 개수를 50, 100, 200으로 달리하여 진행하였으며 그에 따른 결과는 다음과 같다. \n",
    "1. 표에서 보다시피 ANN은 히든레이어의 유닛 개수와 상관없이 훈련을 제대로 진행하지 못하는 반면, RNN(LSTM)은 히든레이어 유닛이 증가함에 따라 정확도가 상승하는 것을 확인할 수 있었다. (실제 error의 감소도 ANN은 변화폭이 거의 없는 반면, RNN(LSTM)은 꾸준히 감소하였다. Accuracy 측면에서 ANN은 오히려 감소하고, RNN(LSTM)은 꾸준히 상승하였다.)\n",
    "2. ANN의 훈련이 안되는 현상은 훈련 데이터(1번)가 잘못 준비되었을수도 있기에 다른 방식(2번)으로 훈련 데이터를 준비하고 재훈련 해보고자 한다. \n",
    " - 훈련 데이터 준비 방식 1 (현재 방식): 4-gram으로 데이터가 준비되었을 경우(e.g., I want to go to school에서 'I','want','to','go') 각 단어를 vocab size에 맞게 one hot coding을 한 뒤 (e.g., vocab size가 1000일 경우 'i'는 1*1000의 one hot coding이 된다.) 각 단어에 1이 적용된 값을 하나의 vocab사이즈에 몰아넣은 뒤 그 값을 훈련값으로 사용한다. (e.g., 'I','want','to'가 각각 vocab size에서 1,2,3번째 값에 1이 매겨졌을 경우[1,0,0,...,0],[0,1,0,0,...,0],[0,0,1,0,...,0] 다음과 같이 [1,1,1,0,...,0]으로 만든다.) 한 vocab size에 3개의 단어가 동시에 담기므로 훈련 데이터의 크기가 n-gram 사이즈에 비례해서 증가하지 않는다.\n",
    " - 훈련 데이터 준비 방식 2 (추가방식): 4-gram으로 데이터가 준비되었을 경우(e.g., I want to go to school에서 'I','want','to','go') 각 단어를 vocab size에 맞게 one hot coding을 한 뒤 (e.g., vocab size가 1000일 경우 'i'는 1*1000의 one hot coding이 된다.) 각 단어에 1이 적용된 값을 모두 이어 붙인 뒤 훈련값으로 사용한다. (e.g., 'I','want','to'가 각각 vocab size에서 1,2,3번째 값에 1이 매겨졌을 경우[1,0,0,...,0],[0,1,0,0,...,0],[0,0,1,0,...,0]을 이어붙여 1*3000의 훈련데이터가 만들어진다.) n-gram 사이즈에 비례해서 훈련데이터가 증가한다. \n",
    "3. 현재의 값이 이전 값과의 연관성을 갖는 text data의 경우 시간상의 정보를 훈련하는 RNN(LSTM)을 이용할 경우 더 좋은 훈련결과를 얻을 수 있음을 본 실험을 통해 알게되었다. \n",
    "\n",
    "|       Model    | Hidden Units  | Accuracy     |\n",
    "| :------------: | :-----------: | -----------: |\n",
    "|   ANN             |       50      |     28.81%   |\n",
    "|   ANN          |       100     |     28.86%   |\n",
    "|   ANN             |       200     |     28.86%   |\n",
    "|RNN(LSTM)       \t     |       50      |     49.76%   |\n",
    "|RNN(LSTM)       |       100     |     56.54%   |\n",
    "|RNN(LSTM)                |       200     |     **72.59%**   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Experiment.\n",
    "1. word 단위의 데이터를 준비하고 실험해보자.\n",
    "2. 데이터 준비 방법 2를 통해 데이터셋을 정제하고 ANN에서 재실험을 해보자.\n",
    "3. 전통적인 방식의 RNN-cell과 LSTM-cell 두 종류를 이용한 RNN의 성능 비교를 해보자.\n",
    "\n",
    "### Github Code\n",
    "Go to the following github and find reports folder. You can run char_ANN_LSTM.py for duplicating the experiment.\n",
    "https://github.com/hyung8758/HY_python_NN.git\n",
    "\n",
    "### Reference\n",
    "\n",
    "- https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/3_NeuralNetworks/dynamic_rnn.py\n",
    "- https://danijar.com/introduction-to-recurrent-networks-in-tensorflow/\n",
    "- https://github.com/jaekookang/useful_bits/blob/dev/Machine_Learning/RNN_LSTM/predict_character/rnn_char.ipynb\n",
    "More but I cannot remember...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
